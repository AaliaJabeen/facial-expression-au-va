---
# Facial Expression to Valence-Arousal Mapping

A PyTorch-based project that detects facial expressions from the FER2013 dataset using a CNN model, maps them to Facial Action Units (AUs), and estimates Valence-Arousal (VA) emotional dimensions.

---

## ğŸ§  Features

- Train a CNN on FER2013 emotion images
- Automatically map predicted emotions to Facial Action Units (AUs)
- Estimate Valence and Arousal levels from AUs
- Clean, modular code with inference support

---

## ğŸ—‚ï¸ Project Structure

```

facial-emotion-va/
â”œâ”€â”€ data/                         # FER2013 dataset (images in subfolders)
â”‚   â”œâ”€â”€ train/angry, happy...
â”‚   â””â”€â”€ test/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ emotion_cnn.pth           # Saved model weights
â”‚   â””â”€â”€ cnn_model.py              # CNN model architecture
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ preprocess.py             # Data loading/transforms
â”‚   â””â”€â”€ va_utils.py               # AU â†’ VA mapping logic
â”œâ”€â”€ mappings/
â”‚   â”œâ”€â”€ emotion_to_au.json
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ EDA_and_Training.ipynb
â”œâ”€â”€ main.py                       # Inference script
â”œâ”€â”€ check_model.py                # Model tester
â””â”€â”€ README.md

````

---

## ğŸ“¦ Setup Instructions

1. **Clone the repo & create environment**

```bash
git clone https://github.com/iammasoodalam/facial-expression-au-va.git
cd facial-expression-au-va
python -m venv .venv
Scripts\activate        # or (in linux) source bin/activate
pip install -r requirements.txt
````

2. **Prepare dataset**

Download the FER2013 image version from:
[https://www.kaggle.com/datasets/msambare/fer2013](https://www.kaggle.com/datasets/msambare/fer2013)

Extract it as:

```
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ angry/
â”‚   â”œâ”€â”€ happy/
â”‚   â””â”€â”€ ...
â””â”€â”€ test/
    â”œâ”€â”€ angry/
    â””â”€â”€ ...
```
or

```bash
python download_dataset.py
```

3. **Train the model**

Run the scripts in the notebook one-by-one

4. **Run inference on a sample image**

Save your image in the data folder, and put the name of the image in check_model.py.

```bash
python check_model.py
```

---

## ğŸ“Š Valence-Arousal Mapping

We use predefined rules to estimate VA scores from Action Units (AUs) based on research literature.

* `emotion â†’ AUs` â†’ defined in `mappings/emotion_to_au.json`
* `AUs â†’ (Valence, Arousal)` â†’ defined in `mappings/au_to_va.json`

---

## ğŸ’¡ Example Output

```bash
ğŸ” Inference Result
------------------------
Image        : sample_face.jpg
Emotion      : happy
Action Units : ['AU6', 'AU12']
Valence      : 0.65
Arousal      : 0.42
```

---

## ğŸ”§ Requirements

* Python 3.8+
* PyTorch
* torchvision
* matplotlib
* opencv-python (optional)
* Jupyter (optional for training)

Install everything using:

```bash
pip install -r requirements.txt
```

---

## ğŸ“Œ Future Improvements

* Add face detection + auto-cropping
* Real-time webcam inference with OpenCV
* Streamlit app for web demo

---

## ğŸ“‘ License

MIT License. Free to use and modify.